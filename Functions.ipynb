{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/opeokupe/capstone-ai-ml/blob/main/Functions.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKXIlhZi5mWX"
   },
   "source": [
    "#Introduction\n",
    "This document includes the functions I will be using for this project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "FgzqGrW15b_M"
   },
   "outputs": [],
   "source": [
    "# 1. Initial Imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_validate, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, f1_score, balanced_accuracy_score, confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from scipy.stats import chi2_contingency, uniform, randint\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "\n",
    "\n",
    "# from google.colab import drive\n",
    "\n",
    "# import sys\n",
    "# sys.path.append('/content/drive/MyDrive/Colab Notebooks/data_exploration.ipynb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "ZDmUhV_Z58U9"
   },
   "outputs": [],
   "source": [
    "def explore_data(df, title=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Performs initial exploration of the dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\n{title} Exploration:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(df.head())\n",
    "    print(\"\\nData Info:\")\n",
    "    print(df.info())\n",
    "    print(\"\\nMissing Values:\")\n",
    "    print(df.isnull().sum())\n",
    "    print(\"\\nBasic Statistics:\")\n",
    "    print(df.describe())\n",
    "\n",
    "\n",
    "def analyze_distributions(df):\n",
    "    \"\"\"\n",
    "    Analyzes and plots distributions of numerical features\n",
    "    :param df: DataFrame\n",
    "    \"\"\"\n",
    "    numerical_cols = df.select_dtypes(include=['int64', 'float64']).columns\n",
    "\n",
    "    plt.figure(figsize=(15, len(numerical_cols)*4))\n",
    "    for i, col in enumerate(numerical_cols, 1):\n",
    "        plt.subplot(len(numerical_cols), 2, i*2-1)\n",
    "        sns.histplot(df[col], kde=True)\n",
    "        plt.title(f'Distribution of {col}')\n",
    "\n",
    "        plt.subplot(len(numerical_cols), 2, i*2)\n",
    "        sns.boxplot(y=df[col])\n",
    "        plt.title(f'Boxplot of {col}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def analyze_non_numeric_distributions(df):\n",
    "    \"\"\"\n",
    "    Analyzes and plots distributions of non-numerical features\n",
    "    :param df: DataFrame\n",
    "    \"\"\"\n",
    "    numerical_cols = df.select_dtypes(exclude=['int64', 'float64']).columns\n",
    "\n",
    "    plt.figure(figsize=(15, len(numerical_cols)*4))\n",
    "    for i, col in enumerate(numerical_cols, 1):\n",
    "        plt.subplot(len(numerical_cols), 2, i*2-1)\n",
    "        sns.histplot(df[col], kde=True)\n",
    "        plt.title(f'Distribution of {col}')\n",
    "\n",
    "        plt.subplot(len(numerical_cols), 2, i*2)\n",
    "        sns.boxplot(y=df[col])\n",
    "        plt.title(f'Boxplot of {col}')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def analyze_non_numeric_distributions2(df):\n",
    "    \"\"\"\n",
    "    Analyzes and plots distributions of non-numerical features with all subplots in one figure\n",
    "    \"\"\"\n",
    "    # Select non-numeric columns (categorical)\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    \n",
    "    if len(categorical_cols) == 0:\n",
    "        print(\"No categorical columns found in the dataset.\")\n",
    "        return\n",
    "\n",
    "    # Calculate number of rows needed (one row per categorical column)\n",
    "    n_rows = len(categorical_cols)\n",
    "    \n",
    "    # Create one large figure for all subplots\n",
    "    fig = plt.figure(figsize=(15, 5*n_rows))\n",
    "    \n",
    "    # Create a color palette with enough colors\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, max(df[categorical_cols].nunique())))\n",
    "\n",
    "    for idx, col in enumerate(categorical_cols):\n",
    "        # Get value counts and calculate percentages\n",
    "        value_counts = df[col].value_counts()\n",
    "        value_percentages = df[col].value_counts(normalize=True) * 100\n",
    "\n",
    "        # Create bar plot\n",
    "        plt.subplot(n_rows, 2, 2*idx + 1)\n",
    "        ax = sns.barplot(x=value_counts.index, y=value_counts.values, \n",
    "                        palette='Set3')\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        # Add percentage labels on top of bars\n",
    "        for i, v in enumerate(value_counts.values):\n",
    "            ax.text(i, v, f'{value_percentages.iloc[i]:.1f}%',\n",
    "                   ha='center', va='bottom')\n",
    "\n",
    "        # Create pie chart\n",
    "        plt.subplot(n_rows, 2, 2*idx + 2)\n",
    "        plt.pie(value_counts.values, labels=value_counts.index,\n",
    "                autopct='%1.1f%%', startangle=90, colors=sns.color_palette(\"Set2\"))\n",
    "        plt.title(f'Percentage Distribution of {col}')\n",
    "\n",
    "\n",
    "    # Adjust layout and display all plots\n",
    "    plt.tight_layout(h_pad=1.5)  # Add some vertical spacing between subplots\n",
    "    plt.show()\n",
    "\n",
    "    # Print summary statistics\n",
    "    for idx, col in enumerate(categorical_cols):\n",
    "        # Print summary statistics\n",
    "        print(f\"\\nSummary for {col}:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Number of unique values: {df[col].nunique()}\")\n",
    "        print(f\"Most common value: {df[col].mode().iloc[0]} ({value_percentages.iloc[0]:.1f}%)\")\n",
    "        print(f\"Missing values: {df[col].isnull().sum()} ({(df[col].isnull().sum()/len(df))*100:.1f}%)\")\n",
    "        print(\"\\nValue Counts:\")\n",
    "        print(pd.DataFrame({\n",
    "            'Count': value_counts,\n",
    "            'Percentage': value_percentages\n",
    "        }))\n",
    "\n",
    "    # Additional analysis for high-cardinality categorical variables\n",
    "    high_cardinality_cols = [col for col in categorical_cols\n",
    "                            if df[col].nunique() > 10]\n",
    "    if high_cardinality_cols:\n",
    "        print(\"\\nHigh Cardinality Columns Analysis:\")\n",
    "        print(\"-\" * 50)\n",
    "        for col in high_cardinality_cols:\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"Number of unique values: {df[col].nunique()}\")\n",
    "            print(\"Top 10 most frequent values:\")\n",
    "            print(df[col].value_counts().head(10))\n",
    "\n",
    "\n",
    "\n",
    "def analyze_non_numeric_distributions(df):\n",
    "    \"\"\"\n",
    "    Analyzes and plots distributions of non-numerical features\n",
    "    \"\"\"\n",
    "    # Select non-numeric columns (categorical)\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    \n",
    "    if len(categorical_cols) == 0:\n",
    "        print(\"No categorical columns found in the dataset.\")\n",
    "        return\n",
    "\n",
    "    # Create figure and subplots for each categorical column\n",
    "    for col in categorical_cols:\n",
    "        # Create a new figure for each column\n",
    "        plt.figure(figsize=(15, 6))\n",
    "        \n",
    "        # Get value counts and calculate percentages\n",
    "        value_counts = df[col].value_counts()\n",
    "        value_percentages = df[col].value_counts(normalize=True) * 100\n",
    "\n",
    "        # Create bar plot\n",
    "        plt.subplot(1, 2, 1)\n",
    "        ax = sns.barplot(x=value_counts.index, y=value_counts.values, palette='Set3')\n",
    "        plt.title(f'Distribution of {col}')\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.ylabel('Count')\n",
    "        \n",
    "        # Add percentage labels on top of bars\n",
    "        for i, v in enumerate(value_counts.values):\n",
    "            ax.text(i, v, f'{value_percentages.iloc[i]:.1f}%',\n",
    "                   ha='center', va='bottom')\n",
    "\n",
    "        # Create pie chart\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.pie(value_counts.values, labels=value_counts.index,\n",
    "                autopct='%1.1f%%', startangle=90)\n",
    "        plt.title(f'Percentage Distribution of {col}')\n",
    "\n",
    "        # Adjust layout and display\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Print summary statistics\n",
    "        print(f\"\\nSummary for {col}:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"Number of unique values: {df[col].nunique()}\")\n",
    "        print(f\"Most common value: {df[col].mode().iloc[0]} ({value_percentages.iloc[0]:.1f}%)\")\n",
    "        print(f\"Missing values: {df[col].isnull().sum()} ({(df[col].isnull().sum()/len(df))*100:.1f}%)\")\n",
    "        print(\"\\nValue Counts:\")\n",
    "        print(pd.DataFrame({\n",
    "            'Count': value_counts,\n",
    "            'Percentage': value_percentages\n",
    "        }))\n",
    "\n",
    "    # Additional analysis for high-cardinality categorical variables\n",
    "    high_cardinality_cols = [col for col in categorical_cols\n",
    "                            if df[col].nunique() > 10]\n",
    "    if high_cardinality_cols:\n",
    "        print(\"\\nHigh Cardinality Columns Analysis:\")\n",
    "        print(\"-\" * 50)\n",
    "        for col in high_cardinality_cols:\n",
    "            print(f\"\\n{col}:\")\n",
    "            print(f\"Number of unique values: {df[col].nunique()}\")\n",
    "            print(\"Top 10 most frequent values:\")\n",
    "            print(df[col].value_counts().head(10))\n",
    "\n",
    "# def analyze_non_numeric_distributions(df):\n",
    "#     \"\"\"\n",
    "#     Analyzes and plots distributions of non-numerical features\n",
    "\n",
    "#     Parameters:\n",
    "#     df (pandas.DataFrame): Input DataFrame\n",
    "#     \"\"\"\n",
    "#     # Select non-numeric columns (categorical)\n",
    "#     categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "#     if len(categorical_cols) == 0:\n",
    "#         print(\"No categorical columns found in the dataset.\")\n",
    "#         return\n",
    "\n",
    "#     # Create subplots for each categorical column\n",
    "#     plt.figure(figsize=(15, 5*len(categorical_cols)))\n",
    "\n",
    "#     for i, col in enumerate(categorical_cols, 1):\n",
    "#         # Get value counts and calculate percentages\n",
    "#         value_counts = df[col].value_counts()\n",
    "#         value_percentages = df[col].value_counts(normalize=True) * 100\n",
    "\n",
    "#         # Create subplot for bar plot\n",
    "#         plt.subplot(len(categorical_cols), 2, i*2-1)\n",
    "#         sns.barplot(x=value_counts.index, y=value_counts.values)\n",
    "#         plt.title(f'Distribution of {col}')\n",
    "#         plt.xticks(rotation=45, ha='right')\n",
    "#         plt.ylabel('Count')\n",
    "\n",
    "#         # Add percentage labels on top of bars - CORRECTED THIS PART\n",
    "#         for idx, (j, v) in enumerate(zip(value_counts.index, value_counts.values)):\n",
    "#             plt.text(idx, v, f'{value_percentages.iloc[idx]:.1f}%',\n",
    "#                      ha='center', va='bottom')\n",
    "\n",
    "#         # Create subplot for pie chart\n",
    "#         plt.subplot(len(categorical_cols), 2, i*2)\n",
    "#         plt.pie(value_counts.values, labels=value_counts.index,\n",
    "#                 autopct='%1.1f%%', startangle=90)\n",
    "#         plt.title(f'Percentage Distribution of {col}')\n",
    "\n",
    "#         # Print summary statistics\n",
    "#         print(f\"\\nSummary for {col}:\")\n",
    "#         print(\"-\" * 50)\n",
    "#         print(f\"Number of unique values: {df[col].nunique()}\")\n",
    "#         print(f\"Most common value: {df[col].mode().iloc[0]} ({value_percentages.iloc[0]:.1f}%)\")\n",
    "#         print(f\"Missing values: {df[col].isnull().sum()} ({(df[col].isnull().sum()/len(df))*100:.1f}%)\")\n",
    "#         print(\"\\nValue Counts:\")\n",
    "#         print(pd.DataFrame({\n",
    "#             'Count': value_counts,\n",
    "#             'Percentage': value_percentages\n",
    "#         }))\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "#     # Additional analysis for high-cardinality categorical variables\n",
    "#     high_cardinality_cols = [col for col in categorical_cols\n",
    "#                              if df[col].nunique() > 10]\n",
    "#     if high_cardinality_cols:\n",
    "#         print(\"\\nHigh Cardinality Columns Analysis:\")\n",
    "#         print(\"-\" * 50)\n",
    "#         for col in high_cardinality_cols:\n",
    "#             print(f\"\\n{col}:\")\n",
    "#             print(f\"Number of unique values: {df[col].nunique()}\")\n",
    "#             print(\"Top 10 most frequent values:\")\n",
    "#             print(df[col].value_counts().head(10))\n",
    "\n",
    "\n",
    "\n",
    "def check_cardinality(df):\n",
    "    \"\"\"\n",
    "    Checks cardinality of categorical columns\n",
    "    \"\"\"\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    cardinality = pd.DataFrame({\n",
    "        'nunique': df[categorical_cols].nunique(),\n",
    "        'percent_unique': df[categorical_cols].nunique() / len(df) * 100\n",
    "    }).sort_values('nunique', ascending=False)\n",
    "\n",
    "    return cardinality\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def analyze_categorical_relationships(df, target_col):\n",
    "    \"\"\"\n",
    "    Analyzes relationships between categorical variables and target using subplots\n",
    "\n",
    "    Parameters:\n",
    "    df (pandas.DataFrame): Input DataFrame\n",
    "    target_col (str): Name of the target column\n",
    "    \"\"\"\n",
    "    # Get categorical columns excluding target\n",
    "    categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "    categorical_cols = [col for col in categorical_cols if col != target_col]\n",
    "\n",
    "    if len(categorical_cols) == 0:\n",
    "        print(\"No categorical columns found in the dataset.\")\n",
    "        return\n",
    "\n",
    "    # Calculate number of rows and columns for subplots\n",
    "    n_cols = 2  # You can adjust this\n",
    "    n_rows = (len(categorical_cols) + n_cols - 1) // n_cols\n",
    "\n",
    "    # Create figure and subplots\n",
    "    fig, axes = plt.subplots(n_rows, n_cols,\n",
    "                             figsize=(15, 5*n_rows),\n",
    "                             squeeze=False)\n",
    "\n",
    "    # Flatten axes array for easier iteration\n",
    "    axes_flat = axes.flatten()\n",
    "\n",
    "    # Create plots\n",
    "    for idx, col in enumerate(categorical_cols):\n",
    "        # Create crosstab\n",
    "        ct = pd.crosstab(df[col], df[target_col], normalize='index')\n",
    "\n",
    "        # Plot on corresponding subplot\n",
    "        ct.plot(kind='bar', ax=axes_flat[idx])\n",
    "\n",
    "        # Customize subplot\n",
    "        axes_flat[idx].set_title(f'{col} vs {target_col}')\n",
    "        axes_flat[idx].set_xlabel(col)\n",
    "        axes_flat[idx].set_ylabel(f'Proportion of {target_col}')\n",
    "        axes_flat[idx].tick_params(axis='x', rotation=45)\n",
    "\n",
    "        # Add percentage labels\n",
    "        for container in axes_flat[idx].containers:\n",
    "            axes_flat[idx].bar_label(container, fmt='%.1f%%',\n",
    "                                     padding=3)\n",
    "\n",
    "        # Print chi-square test results\n",
    "        # chi2, p_value = chi2_contingency(pd.crosstab(df[col], df[target_col]))[:2]\n",
    "        # print(f\"\\nChi-square test results for {col}:\")\n",
    "        # print(f\"Chi-square statistic: {chi2:.2f}\")\n",
    "        # print(f\"p-value: {p_value:.4f}\")\n",
    "\n",
    "    # Remove empty subplots if any\n",
    "    for idx in range(len(categorical_cols), len(axes_flat)):\n",
    "        fig.delaxes(axes_flat[idx])\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print additional statistics\n",
    "    print(\"\\nDetailed Statistics:\")\n",
    "    print(\"-\" * 50)\n",
    "    for col in categorical_cols:\n",
    "        print(f\"\\n{col} breakdown:\")\n",
    "        print(pd.crosstab(df[col], df[target_col],\n",
    "                          normalize='index').round(3) * 100)\n",
    "\n",
    "\n",
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Creates new features based on existing data\n",
    "    :param df: DataFrame\n",
    "    :return: DataFrame with new features\n",
    "    \"\"\"\n",
    "    df_new = df.copy()\n",
    "\n",
    "    # Engagement Score\n",
    "    if 'avg_time_spent' in df.columns and 'points_in_wallet' in df.columns:\n",
    "        df_new['engagement_score'] = df_new['avg_time_spent'] * df_new['points_in_wallet']\n",
    "\n",
    "    # Complaint Ratio\n",
    "    if 'past_complaint' in df.columns and 'complaint_status' in df.columns:\n",
    "        df_new['complaint_resolution_rate'] = (\n",
    "            df_new['complaint_status'].map({'Resolved': 1, 'Not Resolved': 0, 'Pending': 0.5})\n",
    "        )\n",
    "\n",
    "    # Membership Value\n",
    "    df_new['membership_value'] = df_new['membership_category'].map({\n",
    "        'Basic Membership': 1,\n",
    "        'Silver Membership': 2,\n",
    "        'Gold Membership': 3,\n",
    "        'Platinum Membership': 4,\n",
    "        'Premium Membership': 5,\n",
    "        'No Membership': 0\n",
    "    })\n",
    "\n",
    "    # Activity Level\n",
    "    if 'days_since_last_login' in df.columns:\n",
    "        df_new['activity_level'] = pd.cut(\n",
    "            df_new['days_since_last_login'],\n",
    "            bins=[-float('inf'), 7, 30, 90, float('inf')],\n",
    "            labels=['Very Active', 'Active', 'Moderate', 'Inactive']\n",
    "        )\n",
    "\n",
    "    return df_new\n",
    "\n",
    "\n",
    "\n",
    "def preprocess_data(df, is_training=True, fitted_transformers=None, target_col='churn_risk_score'):\n",
    "    \"\"\"\n",
    "    Comprehensive preprocessing pipeline with proper datetime handling\n",
    "    \"\"\"\n",
    "    # Initialize\n",
    "    if fitted_transformers is None:\n",
    "        fitted_transformers = {}\n",
    "\n",
    "    # Copy the dataframe\n",
    "    df_processed = df.copy()\n",
    "\n",
    "    # Separate target if it exists\n",
    "    if target_col in df_processed.columns:\n",
    "        target = df_processed[target_col].copy()\n",
    "        df_processed = df_processed.drop(columns=[target_col])\n",
    "\n",
    "    # 1. First handle date columns\n",
    "    # Convert joining_date to days_since_joining\n",
    "    if 'joining_date' in df_processed.columns:\n",
    "        df_processed['joining_date'] = pd.to_datetime(df_processed['joining_date'])\n",
    "        df_processed['days_since_joining'] = (pd.Timestamp.now() - df_processed['joining_date']).dt.days\n",
    "        df_processed = df_processed.drop('joining_date', axis=1)\n",
    "\n",
    "    # Convert last_visit_time to days_since_last_visit\n",
    "    if 'last_visit_time' in df_processed.columns:\n",
    "        df_processed['last_visit_time'] = pd.to_datetime(df_processed['last_visit_time'])\n",
    "        df_processed['days_since_last_visit'] = (pd.Timestamp.now() - df_processed['last_visit_time']).dt.days\n",
    "        df_processed = df_processed.drop('last_visit_time', axis=1)\n",
    "\n",
    "    # 2. Drop unnecessary columns\n",
    "    id_cols = ['customer_id', 'Name', 'security_no', 'referral_id']\n",
    "    df_processed = df_processed.drop(columns=[col for col in id_cols if col in df_processed.columns])\n",
    "\n",
    "    # 3. Handle avg_frequency_login_days before other processing\n",
    "    if 'avg_frequency_login_days' in df_processed.columns:\n",
    "        # Convert to numeric\n",
    "        df_processed['avg_frequency_login_days'] = pd.to_numeric(df_processed['avg_frequency_login_days'], errors='coerce')\n",
    "\n",
    "        if is_training:\n",
    "            # Store statistics for test set\n",
    "            fitted_transformers['avg_freq_mean'] = df_processed['avg_frequency_login_days'].mean()\n",
    "            fitted_transformers['avg_freq_std'] = df_processed['avg_frequency_login_days'].std()\n",
    "\n",
    "        # Standardize the values\n",
    "        mean = fitted_transformers.get('avg_freq_mean', df_processed['avg_frequency_login_days'].mean())\n",
    "        std = fitted_transformers.get('avg_freq_std', df_processed['avg_frequency_login_days'].std())\n",
    "        df_processed['avg_frequency_login_days'] = (df_processed['avg_frequency_login_days'] - mean) / std\n",
    "\n",
    "    # 4. Handle missing values\n",
    "    numerical_cols = df_processed.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = df_processed.select_dtypes(include=['object', 'category']).columns\n",
    "\n",
    "    for col in numerical_cols:\n",
    "        if df_processed[col].isnull().any():\n",
    "            if is_training:\n",
    "                fitted_transformers[f'{col}_median'] = df_processed[col].median()\n",
    "            df_processed[col] = df_processed[col].fillna(fitted_transformers.get(f'{col}_median', 0))\n",
    "\n",
    "    for col in categorical_cols:\n",
    "        if df_processed[col].isnull().any():\n",
    "            if is_training:\n",
    "                fitted_transformers[f'{col}_mode'] = df_processed[col].mode()[0]\n",
    "            df_processed[col] = df_processed[col].fillna(fitted_transformers.get(f'{col}_mode', 'Unknown'))\n",
    "\n",
    "    # 5. Feature Engineering\n",
    "    if all(col in df_processed.columns for col in ['avg_time_spent', 'points_in_wallet']):\n",
    "        df_processed['engagement_score'] = df_processed['avg_time_spent'] * df_processed['points_in_wallet']\n",
    "\n",
    "    # 6. Encode categorical variables\n",
    "    for col in categorical_cols:\n",
    "        if df_processed[col].nunique() <= 2:\n",
    "            if is_training:\n",
    "                le = LabelEncoder()\n",
    "                df_processed[col] = le.fit_transform(df_processed[col])\n",
    "                fitted_transformers[f'{col}_encoder'] = le\n",
    "            else:\n",
    "                le = fitted_transformers.get(f'{col}_encoder')\n",
    "                if le is not None:\n",
    "                    try:\n",
    "                        df_processed[col] = le.transform(df_processed[col])\n",
    "                    except ValueError:\n",
    "                        df_processed[col] = 0\n",
    "        else:\n",
    "            dummies = pd.get_dummies(df_processed[col], prefix=col)\n",
    "            if is_training:\n",
    "                fitted_transformers[f'{col}_columns'] = dummies.columns.tolist()\n",
    "            else:\n",
    "                expected_columns = fitted_transformers.get(f'{col}_columns', [])\n",
    "                for exp_col in expected_columns:\n",
    "                    if exp_col not in dummies.columns:\n",
    "                        dummies[exp_col] = 0\n",
    "                dummies = dummies[expected_columns]\n",
    "            df_processed = pd.concat([df_processed.drop(col, axis=1), dummies], axis=1)\n",
    "\n",
    "    # 7. Scale remaining numerical features\n",
    "    numerical_cols = df_processed.select_dtypes(include=['int64', 'float64']).columns\n",
    "    if len(numerical_cols) > 0:\n",
    "        if is_training:\n",
    "            scaler = StandardScaler()\n",
    "            fitted_transformers['scaler'] = scaler.fit(df_processed[numerical_cols])\n",
    "\n",
    "        if 'scaler' in fitted_transformers:\n",
    "            df_processed[numerical_cols] = fitted_transformers['scaler'].transform(df_processed[numerical_cols])\n",
    "\n",
    "    # 8. Ensure column consistency\n",
    "    if not is_training and 'training_columns' in fitted_transformers:\n",
    "        missing_cols = set(fitted_transformers['training_columns']) - set(df_processed.columns)\n",
    "        for col in missing_cols:\n",
    "            df_processed[col] = 0\n",
    "        df_processed = df_processed[fitted_transformers['training_columns']]\n",
    "    elif is_training:\n",
    "        fitted_transformers['training_columns'] = df_processed.columns.tolist()\n",
    "\n",
    "    # Add back target\n",
    "    if target_col in df.columns:\n",
    "        df_processed[target_col] = target\n",
    "\n",
    "    return df_processed, fitted_transformers\n",
    "\n",
    "\n",
    "\n",
    "def create_model_pipeline(y=None):\n",
    "    \"\"\"\n",
    "    Create a dictionary of models suitable for imbalanced classification\n",
    "    \"\"\"\n",
    "    # Calculate class weights if y is provided\n",
    "    if y is not None:\n",
    "        class_weights = get_class_weights(y)\n",
    "    else:\n",
    "        class_weights = None\n",
    "\n",
    "    n_classes = len(np.unique(y)) if y is not None else 6\n",
    "\n",
    "    models = {\n",
    "        'xgboost': xgb.XGBClassifier(\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=200,\n",
    "            max_depth=6,\n",
    "            min_child_weight=1,\n",
    "            gamma=0,\n",
    "            subsample=0.8,\n",
    "            colsample_bytree=0.8,\n",
    "            scale_pos_weight=1,\n",
    "            enable_categorical=True,\n",
    "            objective='multi:softmax',\n",
    "            num_class=n_classes,  # For classes 0-5\n",
    "            random_state=42,\n",
    "            tree_method='hist',\n",
    "            seed=42,\n",
    "            class_weight=class_weights\n",
    "        ),\n",
    "\n",
    "        'lightgbm': lgb.LGBMClassifier(\n",
    "            objective='multiclass',\n",
    "            num_class=n_classes,\n",
    "            learning_rate=0.1,\n",
    "            n_estimators=300,\n",
    "            max_depth=8,\n",
    "            num_leaves=31,\n",
    "            class_weight=class_weights,\n",
    "            random_state=42,\n",
    "            verbose=-1,\n",
    "            boosting_type='gbdt',\n",
    "            is_unbalance=True\n",
    "        ),\n",
    "\n",
    "        'random_forest': RandomForestClassifier(\n",
    "            n_estimators=300,\n",
    "            max_depth=8,\n",
    "            class_weight=class_weights if class_weights else 'balanced',\n",
    "            random_state=42\n",
    "        )\n",
    "    }\n",
    "\n",
    "    return models\n",
    "\n",
    "def custom_metrics(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate multiple classification metrics\n",
    "    \"\"\"\n",
    "    metrics = {}\n",
    "    metrics['weighted_f1'] = f1_score(y_true, y_pred, average='weighted')\n",
    "    metrics['balanced_accuracy'] = balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "    # Calculate confusion matrix and derived metrics\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    metrics['confusion_matrix'] = cm\n",
    "\n",
    "    # Per-class metrics\n",
    "    metrics['per_class_precision'] = np.diag(cm) / np.sum(cm, axis=0)\n",
    "    metrics['per_class_recall'] = np.diag(cm) / np.sum(cm, axis=1)\n",
    "\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_confusion_matrices(y_true, predictions):\n",
    "    \"\"\"\n",
    "    Plot confusion matrices. Can handle both dictionary of predictions and single prediction array.\n",
    "    \"\"\"\n",
    "    if not isinstance(predictions, dict):\n",
    "        # If single array is passed, convert to dictionary\n",
    "        predictions = {'model': predictions}\n",
    "\n",
    "    n_models = len(predictions)\n",
    "    fig, axes = plt.subplots(1, n_models, figsize=(5*n_models, 4))\n",
    "\n",
    "    # Convert axes to array if single plot\n",
    "    if n_models == 1:\n",
    "        axes = np.array([axes])\n",
    "\n",
    "    for ax, (name, y_pred) in zip(axes, predictions.items()):\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        sns.heatmap(cm, annot=True, fmt='d', ax=ax)\n",
    "        ax.set_title(f'{name} Confusion Matrix')\n",
    "        ax.set_xlabel('Predicted')\n",
    "        ax.set_ylabel('True')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_roc_curves(y_true, predictions_proba_dict):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "\n",
    "    for name, y_pred_proba in predictions_proba_dict.items():\n",
    "        fpr, tpr, _ = roc_curve(y_true, y_pred_proba)\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        plt.plot(fpr, tpr, label=f'{name} (AUC = {roc_auc:.2f})')\n",
    "\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curves')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def print_classification_reports(y_true, predictions):\n",
    "    \"\"\"\n",
    "    Print classification reports. Can handle both dictionary of predictions and single prediction array.\n",
    "    \"\"\"\n",
    "    reports = {}\n",
    "\n",
    "    if not isinstance(predictions, dict):\n",
    "        # If single array is passed, convert to dictionary\n",
    "        predictions = {'model': predictions}\n",
    "\n",
    "    for name, y_pred in predictions.items():\n",
    "        print(f\"\\n{name} Classification Report:\")\n",
    "        print(classification_report(y_true, y_pred))\n",
    "        reports[name] = classification_report(y_true, y_pred, output_dict=True)\n",
    "\n",
    "    return reports\n",
    "\n",
    "\n",
    "def compare_model_metrics(reports):\n",
    "    metrics = ['precision', 'recall', 'f1-score']\n",
    "    comparison = {}\n",
    "\n",
    "    for model, report in reports.items():\n",
    "        comparison[model] = {metric: report['weighted avg'][metric]\n",
    "                             for metric in metrics}\n",
    "\n",
    "    return pd.DataFrame(comparison).round(3)\n",
    "\n",
    "\n",
    "def train_evaluate_models(X, y, models, n_splits=5):\n",
    "    \"\"\"\n",
    "    Train and evaluate models using both cross-validation and train-test split\n",
    "    \"\"\"\n",
    "    print(\"Shape of X:\", X.shape)\n",
    "    print(\"Unique classes in y:\", np.unique(y))\n",
    "    print(\"Class distribution:\", pd.Series(y).value_counts().sort_index())\n",
    "\n",
    "    # Convert categorical columns to numeric\n",
    "    X_processed = X.copy()\n",
    "    categorical_columns = X_processed.select_dtypes(include=['object']).columns\n",
    "\n",
    "    print(\"\\nCategorical columns being processed:\", categorical_columns.tolist())\n",
    "\n",
    "    # Convert each categorical column\n",
    "    for col in categorical_columns:\n",
    "        if X_processed[col].nunique() == 2:\n",
    "            le = LabelEncoder()\n",
    "            X_processed[col] = le.fit_transform(X_processed[col])\n",
    "        else:\n",
    "            dummies = pd.get_dummies(X_processed[col], prefix=col, drop_first=True)\n",
    "            X_processed = pd.concat([X_processed.drop(col, axis=1), dummies], axis=1)\n",
    "\n",
    "    # Split data for validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_processed, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    results = {}\n",
    "    feature_importance = {}\n",
    "    validation_results = {}\n",
    "\n",
    "    for name, model in models.items():\n",
    "        print(f\"\\nTraining {name}...\")\n",
    "\n",
    "        try:\n",
    "            # Train on training set\n",
    "            model.fit(X_train, y_train)\n",
    "\n",
    "            # Make predictions on validation set\n",
    "            val_predictions = model.predict(X_val)\n",
    "\n",
    "            # Calculate validation metrics\n",
    "            validation_results[name] = {\n",
    "                'accuracy': accuracy_score(y_val, val_predictions),\n",
    "                'weighted_f1': f1_score(y_val, val_predictions, average='weighted'),\n",
    "                'confusion_matrix': confusion_matrix(y_val, val_predictions)\n",
    "            }\n",
    "\n",
    "            # Plot confusion matrix\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            sns.heatmap(validation_results[name]['confusion_matrix'],\n",
    "                        annot=True, fmt='d', cmap='Blues')\n",
    "            plt.title(f'{name} Validation Confusion Matrix')\n",
    "            plt.xlabel('Predicted')\n",
    "            plt.ylabel('True')\n",
    "            plt.show()\n",
    "\n",
    "            # Cross-validation\n",
    "            cv_results = cross_validate(\n",
    "                model, X_processed, y,\n",
    "                cv=StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42),\n",
    "                scoring={\n",
    "                    'weighted_f1': make_scorer(f1_score, average='weighted'),\n",
    "                    'balanced_accuracy': make_scorer(balanced_accuracy_score)\n",
    "                },\n",
    "                return_train_score=True,\n",
    "                n_jobs=-1\n",
    "            )\n",
    "\n",
    "            # Store CV results\n",
    "            results[name] = {\n",
    "                'test_weighted_f1_mean': cv_results['test_weighted_f1'].mean(),\n",
    "                'test_weighted_f1_std': cv_results['test_weighted_f1'].std(),\n",
    "                'test_balanced_accuracy_mean': cv_results['test_balanced_accuracy'].mean(),\n",
    "                'test_balanced_accuracy_std': cv_results['test_balanced_accuracy'].std(),\n",
    "            }\n",
    "\n",
    "            # Get feature importance\n",
    "            if hasattr(model, 'feature_importances_'):\n",
    "                feature_importance[name] = pd.Series(\n",
    "                    model.feature_importances_,\n",
    "                    index=X_processed.columns\n",
    "                ).sort_values(ascending=False)\n",
    "\n",
    "            # Print validation results\n",
    "            print(f\"\\nValidation Results for {name}:\")\n",
    "            print(f\"Accuracy: {validation_results[name]['accuracy']:.4f}\")\n",
    "            print(f\"Weighted F1: {validation_results[name]['weighted_f1']:.4f}\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error training {name}: {str(e)}\")\n",
    "            continue\n",
    "\n",
    "    return results, feature_importance, validation_results\n",
    "\n",
    "\n",
    "def create_ensemble(models, voting='soft'):\n",
    "    \"\"\"\n",
    "    Create an ensemble model using voting\n",
    "    \"\"\"\n",
    "    estimators = [(name, model) for name, model in models.items()]\n",
    "    ensemble = VotingClassifier(estimators=estimators, voting=voting)\n",
    "    return ensemble\n",
    "\n",
    "\n",
    "def plot_feature_importance(feature_importance_dict):\n",
    "    \"\"\"\n",
    "    Plot feature importance for each model\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    for i, (name, importance) in enumerate(feature_importance_dict.items(), 1):\n",
    "        plt.subplot(len(feature_importance_dict), 1, i)\n",
    "        importance.head(20).plot(kind='barh')\n",
    "        plt.title(f'Top 20 Important Features - {name}')\n",
    "        plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def run_model_pipeline(X_train, y_train):\n",
    "    \"\"\"\n",
    "    Run the complete modeling pipeline\n",
    "    \"\"\"\n",
    "    # Create models\n",
    "    models = create_model_pipeline(y_train)\n",
    "\n",
    "    # Train and evaluate individual models\n",
    "    print(\"Training and evaluating individual models...\")\n",
    "    results, feature_importance, validation_results = train_evaluate_models(X_train, y_train, models)\n",
    "\n",
    "    # Access validation results\n",
    "    for name, metrics in validation_results.items():\n",
    "        print(f\"\\nValidation metrics for {name}:\")\n",
    "        print(f\"Accuracy: {metrics['accuracy']:.4f}\")\n",
    "        print(f\"Weighted F1: {metrics['weighted_f1']:.4f}\")\n",
    "\n",
    "    # Print results\n",
    "    print(\"\\nModel Performance Summary:\")\n",
    "    print(\"-\" * 50)\n",
    "    for name, metrics in results.items():\n",
    "        print(f\"\\n{name} Results:\")\n",
    "        for metric, value in metrics.items():\n",
    "            print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    # Create and evaluate ensemble\n",
    "    print(\"\\nTraining ensemble model...\")\n",
    "    ensemble = create_ensemble(models)\n",
    "    ensemble_results, _, ensemble_validation = train_evaluate_models(\n",
    "        X_train, y_train, {'ensemble': ensemble}\n",
    "    )\n",
    "\n",
    "    # Plot feature importance\n",
    "    print(\"\\nPlotting feature importance...\")\n",
    "    plot_feature_importance(feature_importance)\n",
    "\n",
    "    return results, feature_importance, ensemble\n",
    "\n",
    "\n",
    "def tune_xgboost(X, y):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for XGBoost with proper categorical handling\n",
    "    \"\"\"\n",
    "    # Process categorical columns first\n",
    "    X_processed = X.copy()\n",
    "    categorical_columns = X_processed.select_dtypes(include=['object']).columns\n",
    "\n",
    "    print(f\"Processing categorical columns: {list(categorical_columns)}\")\n",
    "\n",
    "    # Convert categorical columns\n",
    "    for col in categorical_columns:\n",
    "        if X_processed[col].nunique() == 2:\n",
    "            # Binary categories\n",
    "            le = LabelEncoder()\n",
    "            X_processed[col] = le.fit_transform(X_processed[col])\n",
    "        else:\n",
    "            # Multi-class categories\n",
    "            dummies = pd.get_dummies(X_processed[col], prefix=col, drop_first=True)\n",
    "            X_processed = pd.concat([X_processed.drop(col, axis=1), dummies], axis=1)\n",
    "\n",
    "    print(\"Shape after processing:\", X_processed.shape)\n",
    "    print(\"Data types after processing:\\n\", X_processed.dtypes)\n",
    "\n",
    "    param_grid = {\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'learning_rate': [0.01, 0.1],\n",
    "        'n_estimators': [100, 200],\n",
    "        'min_child_weight': [1, 3],\n",
    "        'gamma': [0, 0.1],\n",
    "        'subsample': [0.8, 0.9],\n",
    "        'colsample_bytree': [0.8, 0.9],\n",
    "        'tree_method': ['hist'],  # Use histogram-based algorithm\n",
    "        'objective': ['multi:softmax'],\n",
    "        'num_class': [len(np.unique(y))]\n",
    "    }\n",
    "\n",
    "    xgb_model = xgb.XGBClassifier(random_state=42)\n",
    "\n",
    "    search = RandomizedSearchCV(\n",
    "        xgb_model,\n",
    "        param_grid,\n",
    "        n_iter=10,\n",
    "        scoring='f1_weighted',\n",
    "        n_jobs=-1,\n",
    "        cv=StratifiedKFold(n_splits=5),\n",
    "        random_state=42,\n",
    "        error_score='raise'  # This will help with debugging\n",
    "    )\n",
    "\n",
    "    try:\n",
    "        search.fit(X_processed, y)\n",
    "        print(\"Best parameters found:\", search.best_params_)\n",
    "        return search.best_params_\n",
    "    except Exception as e:\n",
    "        print(\"Error during tuning:\", str(e))\n",
    "        print(\"Falling back to default parameters\")\n",
    "        return {\n",
    "            'max_depth': 5,\n",
    "            'learning_rate': 0.1,\n",
    "            'n_estimators': 100,\n",
    "            'min_child_weight': 1,\n",
    "            'gamma': 0,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'tree_method': 'hist',\n",
    "            'objective': 'multi:softmax',\n",
    "            'num_class': len(np.unique(y))\n",
    "        }\n",
    "\n",
    "\n",
    "\n",
    "def tune_lightgbm(X, y):\n",
    "    \"\"\"\n",
    "    Perform hyperparameter tuning for LightGBM with proper categorical handling\n",
    "    \"\"\"\n",
    "    # Process categorical columns first\n",
    "    X_processed = X.copy()\n",
    "    categorical_columns = X_processed.select_dtypes(include=['object']).columns\n",
    "\n",
    "    print(f\"Processing categorical columns: {list(categorical_columns)}\")\n",
    "\n",
    "    # Convert categorical columns\n",
    "    for col in categorical_columns:\n",
    "        if X_processed[col].nunique() == 2:\n",
    "            # Binary categories\n",
    "            le = LabelEncoder()\n",
    "            X_processed[col] = le.fit_transform(X_processed[col])\n",
    "        else:\n",
    "            # Multi-class categories\n",
    "            dummies = pd.get_dummies(X_processed[col], prefix=col, drop_first=True)\n",
    "            X_processed = pd.concat([X_processed.drop(col, axis=1), dummies], axis=1)\n",
    "\n",
    "    print(\"Shape after processing:\", X_processed.shape)\n",
    "    print(\"Data types after processing:\\n\", X_processed.dtypes)\n",
    "\n",
    "    param_grid = {\n",
    "        'learning_rate': uniform(0.01, 0.3),\n",
    "        'n_estimators': randint(100, 500),\n",
    "        'max_depth': randint(3, 10),\n",
    "        'num_leaves': randint(20, 100),\n",
    "        'subsample': uniform(0.6, 0.4),\n",
    "        'colsample_bytree': uniform(0.6, 0.4),\n",
    "        'objective': ['multiclass'],\n",
    "        'num_class': [len(np.unique(y))]\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        search = RandomizedSearchCV(\n",
    "            estimator=lgb.LGBMClassifier(verbose=-1, objective='multiclass'),\n",
    "            param_distributions=param_grid,\n",
    "            n_iter=20,\n",
    "            cv=5,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "            error_score='raise'\n",
    "        )\n",
    "\n",
    "        search.fit(X_processed, y)\n",
    "        print(\"Best parameters found:\", search.best_params_)\n",
    "        return search.best_params_\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"Error during tuning:\", str(e))\n",
    "        print(\"Falling back to default parameters\")\n",
    "        return {\n",
    "            'learning_rate': 0.1,\n",
    "            'n_estimators': 200,\n",
    "            'max_depth': 6,\n",
    "            'num_leaves': 31,\n",
    "            'subsample': 0.8,\n",
    "            'colsample_bytree': 0.8,\n",
    "            'objective': 'multiclass',\n",
    "            'num_class': len(np.unique(y))\n",
    "        }\n",
    "\n",
    "\n",
    "# def tune_lightgbm(X, y):\n",
    "#     param_grid = {\n",
    "#         'learning_rate': uniform(0.01, 0.3),\n",
    "#         'n_estimators': randint(100, 500),\n",
    "#         'max_depth': randint(3, 10),\n",
    "#         'num_leaves': randint(20, 100),\n",
    "#         'subsample': uniform(0.6, 0.4),\n",
    "#         'colsample_bytree': uniform(0.6, 0.4)\n",
    "#     }\n",
    "#\n",
    "#     search = RandomizedSearchCV(\n",
    "#         estimator=lgb.LGBMClassifier(),\n",
    "#         param_distributions=param_grid,\n",
    "#         n_iter=20,\n",
    "#         cv=5,\n",
    "#         random_state=42,\n",
    "#         n_jobs=-1\n",
    "#     )\n",
    "#\n",
    "#     search.fit(X, y)\n",
    "#     return search.best_params_\n",
    "\n",
    "def create_optimized_ensemble(models, X, y):\n",
    "    \"\"\"Create a voting classifier with optimized weights\"\"\"\n",
    "    from sklearn.ensemble import VotingClassifier\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "\n",
    "    # Get base predictions\n",
    "    predictions = {}\n",
    "    for name, model in models.items():\n",
    "        predictions[name] = cross_val_score(model, X, y, cv=5, scoring='accuracy')\n",
    "\n",
    "    # Calculate weights based on mean CV scores\n",
    "    weights = {name: scores.mean() for name, scores in predictions.items()}\n",
    "    total = sum(weights.values())\n",
    "    weights = [w/total for w in weights.values()]\n",
    "\n",
    "    # Create voting classifier\n",
    "    estimators = [(name, model) for name, model in models.items()]\n",
    "    ensemble = VotingClassifier(\n",
    "        estimators=estimators,\n",
    "        weights=weights,\n",
    "        voting='soft'\n",
    "    )\n",
    "\n",
    "    return ensemble\n",
    "\n",
    "\n",
    "def get_class_weights(y):\n",
    "    classes = np.unique(y)\n",
    "    weights = compute_class_weight('balanced', classes=classes, y=y)\n",
    "    return dict(zip(classes, weights))\n",
    "\n",
    "\n",
    "def prepare_data_for_smote(X, y):\n",
    "    \"\"\"\n",
    "    Prepare data for SMOTE by handling NaN values and categorical variables\n",
    "    \"\"\"\n",
    "    X_processed = X.copy()\n",
    "\n",
    "    # First handle NaN values\n",
    "    numeric_cols = X_processed.select_dtypes(include=['int64', 'float64']).columns\n",
    "    categorical_cols = X_processed.select_dtypes(include=['object']).columns\n",
    "\n",
    "    print(\"Handling missing values...\")\n",
    "    print(\"Numeric columns:\", numeric_cols.tolist())\n",
    "    print(\"Categorical columns:\", categorical_cols.tolist())\n",
    "\n",
    "    # Handle numeric columns\n",
    "    for col in numeric_cols:\n",
    "        median_val = X_processed[col].median()\n",
    "        X_processed[col] = X_processed[col].fillna(median_val)\n",
    "\n",
    "    # Handle categorical columns\n",
    "    for col in categorical_cols:\n",
    "        # Replace 'Error' values with NaN first\n",
    "        X_processed[col] = X_processed[col].replace('Error', np.nan)\n",
    "        # Then fill NaN with mode\n",
    "        mode_val = X_processed[col].mode()[0]\n",
    "        X_processed[col] = X_processed[col].fillna(mode_val)\n",
    "\n",
    "        # Encode categorical variables\n",
    "        if X_processed[col].nunique() == 2:\n",
    "            le = LabelEncoder()\n",
    "            X_processed[col] = le.fit_transform(X_processed[col])\n",
    "        else:\n",
    "            dummies = pd.get_dummies(X_processed[col], prefix=col, drop_first=True)\n",
    "            X_processed = pd.concat([X_processed.drop(col, axis=1), dummies], axis=1)\n",
    "\n",
    "    print(\"\\nData shape after processing:\", X_processed.shape)\n",
    "    print(\"Any remaining NaN values:\", X_processed.isnull().sum().sum())\n",
    "\n",
    "    return X_processed, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xsBQ_AXo5g-l"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPDlbLuYCDMWNGHET+teotn",
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
